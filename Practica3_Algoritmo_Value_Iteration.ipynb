{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo Value Iteration\n",
    "En el presente código se implementa el algoritmo **Value Iteration** para la resolución del entorno 'Frozen Lake v1'. Se implementan dos versiones, la primera con un agente que tiene acceso a la función de transición y a las recompensas del entorno, y un segundo que no tiene este acceso, por lo que debe realizar estimaciones.\n",
    "\n",
    "**NOTA IMPORTANTE:** Se logró reconocer que el método implementado en el libro falla en ocasiones; pues se \"atora\" en alguna acción específica, impidiéndole explorar nuevas acciones y estados, lo que provoca que el entrenamiento del agente entre a un bucle infinito (o demasiado largo). Para evitar esto, se inicializaron los valores para el primer *agente con conocimiento* en valores aleatorios, pero aún así no se logra evitar del todo. En fin, si se corre este código y alguna celda de entrenamiento tarda demasiado (más de un minuto), probablemente se haya estancado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideraciones del entorno y ecuaciones que se usarán"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo **Value Iteration** permite encontrar los valores óptimos V(s) para cada estado, de tal manera que al calcular el valor de acción máximo Q(s) de cada estado se pueda elegir la mejor acción en ese estado.\n",
    "\n",
    "Consideraciones:\n",
    "\n",
    "El entorno para el que se implementa el método en el presente documento, es para el entorno 'Frozen Lake v1' en su modo *slippery*, por lo que se deduce que es un entorno estocástico con políticas deterministas. Más concretamente:\n",
    "\n",
    "- Política: 0.25 de probabilidad de elegir cualquiera de las 4 acciones (0, 1, 2, 3)\n",
    "- Entorno: Posibilidad de caer en dos estados diferentes más a parte del principal, con una probabilidad de 0.33333 para cada uno\n",
    "\n",
    "De aquí se deduce que las ecuaciones a usar para encontrar el V*(s) óptimo de cada estado son las siguientes, dadas su política determinista y entorno estocástico:\n",
    "\n",
    "La ecuación de valor de acción (para entornos estocásticos) es:\n",
    "$$ Q(s) = \\sum_{s'}^{}P(s'|s,a) [R(s,a,s')+\\gamma V(s')] $$\n",
    "\n",
    "La ecuación de valor de estado será por tanto: \n",
    "$$ V_{*}(s) = max_{a}Q_{*}(s,a)$$\n",
    "\n",
    "Se puede observar en esta última, que se requiere el valor de $Q_{*}$, si se revisa el libro se puede observar que la ecuación de valor de acción que se usa $ Q_{s}$ es equivalente a la función de valor de acción óptima $ Q_{*} $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente con conocimiento del entorno"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clases y funciones a usar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El agente en este notebook ya incorpora el método **Value Iteration** para aprender a resolver el entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def __init__(self, GAMMA, size_ambiente=\"4x4\"):\n",
    "        self.env = gym.make('FrozenLake-v1', is_slippery=True, map_name=size_ambiente)\n",
    "        self.GAMMA = GAMMA\n",
    "        self.V = np.random.rand(self.env.observation_space.n) # Esta variable contendrá el valor de cada estado\n",
    "    \n",
    "    def calcular_Q(self, s, a): # s=estado, a=acción\n",
    "        # Calcular Q(s)\n",
    "        valor_accion = sum([probabilidad*(recompensa + self.GAMMA*self.V[siguiente_estado]) for probabilidad, siguiente_estado, recompensa, _ in self.env.P[s][a]])\n",
    "        return valor_accion\n",
    "    \n",
    "    def seleccionar_accion(self, s):\n",
    "        mejor_accion = mejor_valor = False\n",
    "        \n",
    "        for accion in range(self.env.action_space.n): # Para cada acción (son 4)\n",
    "            valor_accion = self.calcular_Q(s, accion)\n",
    "            if mejor_valor is False or mejor_valor < valor_accion: # Si no hay mejor valor o el valor de la acción es mayor que el mejor valor\n",
    "                mejor_valor = valor_accion\n",
    "                mejor_accion = accion\n",
    "\n",
    "        return mejor_accion\n",
    "    \n",
    "    def Value_Iteration(self):\n",
    "        for estado in range(self.env.observation_space.n):\n",
    "            # Calcular V*(s)\n",
    "            valores_estado = [self.calcular_Q(estado, accion) for accion in range(self.env.action_space.n)] \n",
    "            self.V[estado] = max(valores_estado)\n",
    "            \n",
    "        return self.V"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función corre un número de episodios y regresa el porcentaje de victorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revisar_mejoras(agente, no_episodios = 20, size_ambiente=\"4x4\"):\n",
    "    ambiente_test = gym.make('FrozenLake-v1', is_slippery=True, render_mode='rgb_array', map_name=size_ambiente)\n",
    "    recompensa_total_episodios = 0\n",
    "    \n",
    "    for n in range(no_episodios):\n",
    "        recompensa_episodio = 0\n",
    "        estado = ambiente_test.reset()[0]\n",
    "        while True:\n",
    "            accion = agente.seleccionar_accion(estado)\n",
    "            new_estado, new_recompensa, is_done, truncado, info = ambiente_test.step(accion)\n",
    "            recompensa_episodio += new_recompensa\n",
    "            if is_done is True:\n",
    "                break\n",
    "            estado = new_estado\n",
    "        recompensa_total_episodios += recompensa_episodio\n",
    "    recompensa_total_episodios /= no_episodios\n",
    "    return recompensa_total_episodios\n",
    "\n",
    "# Revisar mejoras con render humano\n",
    "def revisar_mejoras_render(agente, no_episodios = 20, size_ambiente=\"4x4\"):\n",
    "    ambiente_test = gym.make('FrozenLake-v1', is_slippery=True, render_mode='human', map_name=size_ambiente)\n",
    "    recompensa_total_episodios = 0\n",
    "    \n",
    "    for n in range(no_episodios):\n",
    "        recompensa_episodio = 0\n",
    "        estado = ambiente_test.reset()[0]\n",
    "        while True:\n",
    "            accion = agente.seleccionar_accion(estado)\n",
    "            new_estado, new_recompensa, is_done, truncado, info = ambiente_test.step(accion)\n",
    "            recompensa_episodio += new_recompensa\n",
    "            if is_done is True:\n",
    "                break\n",
    "            estado = new_estado\n",
    "        recompensa_total_episodios += recompensa_episodio\n",
    "    recompensa_total_episodios /= no_episodios\n",
    "    return recompensa_total_episodios"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para extraer la política aprendida del agente y para poder visualizarla mejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer política\n",
    "def extraer_politica(agente):\n",
    "    politica = np.zeros(agente.env.observation_space.n)\n",
    "    for estado in range(agente.env.observation_space.n):\n",
    "        politica[estado] = agente.seleccionar_accion(estado) # En este caso se selecciona la acción que se realiza siempre en ese estado\n",
    "                                                                # con los actuales valores de estado V\n",
    "    return politica\n",
    "\n",
    "# Visualizar política\n",
    "def visualizar_politica(politica):\n",
    "    print(politica.reshape(-1,4))\n",
    "    print('\\n')\n",
    "    ayuda_visual = {0:'<', 1:'v', 2:'>', 3:'^'}\n",
    "    flechas_politica = [ayuda_visual[x] for x in politica]\n",
    "    print(np.array(flechas_politica).reshape(-1,4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor recompensa actualizada 0.90 en la iteración 1\n"
     ]
    }
   ],
   "source": [
    "UMBRAL_RECOMPENSA = 0.9\n",
    "t = 0\n",
    "mejor_recompensa = 0\n",
    "agente = Agente(GAMMA=0.95)\n",
    "\n",
    "# BUCLE DE ENTRENAMIENTO\n",
    "while mejor_recompensa < UMBRAL_RECOMPENSA:\n",
    "    agente.Value_Iteration()\n",
    "    t += 1\n",
    "    recompensa_total_episodios = revisar_mejoras(agente)\n",
    "    \n",
    "    if recompensa_total_episodios > mejor_recompensa:\n",
    "        print(f\"Mejor recompensa actualizada {recompensa_total_episodios:.2f} en la iteración {t}\")\n",
    "        mejor_recompensa = recompensa_total_episodios"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valores de estado finales calculados por el método **Value Iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62712855 0.51930681 0.63051944 0.6422135  0.76176898 0.02751469\n",
      " 0.77339044 0.84516176 0.657069   0.78639335 0.66925934 0.33776129\n",
      " 0.353127   0.71066851 1.03836723 0.80417024]\n"
     ]
    }
   ],
   "source": [
    "print(agente.V)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis del comportamiento adquirido"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar la política adquirida, observar que es una política determinista. Esto gracias a que se calculó el valor de estado óptimo para cada estado, de tal manera que al seleccionar la acción con el valor máximo en ese estado concreto, siempre fuera la misma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 3. 2. 2.]\n",
      " [0. 0. 2. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "\n",
      "\n",
      "[['<' '^' '>' '>']\n",
      " ['<' '<' '>' '<']\n",
      " ['^' 'v' '<' '<']\n",
      " ['<' '>' 'v' '<']]\n"
     ]
    }
   ],
   "source": [
    "politica = extraer_politica(agente)\n",
    "visualizar_politica(politica)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar algunos episodios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revisar_mejoras_render(agente)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis con Tensorboard\n",
    "Esta parte del código sólo funciona en Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agente2 = Agente(GAMMA=0.95)\n",
    "# t = 0\n",
    "# mejor_recompensa = 0\n",
    "\n",
    "# while mejor_recompensa < UMBRAL_RECOMPENSA:\n",
    "#     agente2.Value_Iteration()\n",
    "#     t += 1\n",
    "#     recompensa_total_episodios = revisar_mejoras(agente2)\n",
    "#     writer.add_scalar('Recompensa', recompensa_total_episodios, t)\n",
    "#     if recompensa_total_episodios > mejor_recompensa:\n",
    "#         mejor_recompensa = recompensa_total_episodios\n",
    "        \n",
    "# writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comportamiento del agente en un entorno más grande (Frozen Lake v1 8x8)\n",
    "**NOTA IMPORTANTE:** Cabe destacar que para que el entrenamiento no se quedara en un bucle infinito para este entorno, se tuvo que hacer un cambio a la clase, e inicializar los valores de estado **V** aleatoriamente y no en ceros como decía en el libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor recompensa actualizada 0.20 en la iteración 25\n",
      "Mejor recompensa actualizada 0.40 en la iteración 28\n",
      "Mejor recompensa actualizada 0.45 en la iteración 29\n",
      "Mejor recompensa actualizada 0.50 en la iteración 30\n",
      "Mejor recompensa actualizada 0.65 en la iteración 36\n",
      "Mejor recompensa actualizada 0.70 en la iteración 43\n",
      "Mejor recompensa actualizada 0.75 en la iteración 44\n",
      "Mejor recompensa actualizada 0.85 en la iteración 51\n",
      "Mejor recompensa actualizada 0.95 en la iteración 71\n"
     ]
    }
   ],
   "source": [
    "agenteGrande = Agente(GAMMA=0.95, size_ambiente=\"8x8\")\n",
    "UMBRAL_RECOMPENSA = 0.9\n",
    "t = 0\n",
    "mejor_recompensa = 0\n",
    "\n",
    "# BUCLE DE ENTRENAMIENTO\n",
    "while mejor_recompensa < UMBRAL_RECOMPENSA:\n",
    "    agenteGrande.Value_Iteration()\n",
    "    t += 1\n",
    "    recompensa_total_episodios = revisar_mejoras(agenteGrande, size_ambiente=\"8x8\")\n",
    "    \n",
    "    if recompensa_total_episodios > mejor_recompensa:\n",
    "        print(f\"Mejor recompensa actualizada {recompensa_total_episodios:.2f} en la iteración {t}\")\n",
    "        mejor_recompensa = recompensa_total_episodios"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correr algunos episodios con render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revisar_mejoras_render(agenteGrande, size_ambiente=\"8x8\", no_episodios=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente con estimaciones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clases y funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class AgenteEstimaciones:\n",
    "    def __init__(self, GAMMA, size_ambiente=\"4x4\"):\n",
    "        self.env = gym.make('FrozenLake-v1', is_slippery=True, render_mode='rgb_array', map_name=size_ambiente)\n",
    "        self.GAMMA = GAMMA\n",
    "        self.state = self.env.reset()[0]\n",
    "        self.recompensas = collections.defaultdict(float)\n",
    "        self.transiciones = collections.defaultdict(collections.Counter)\n",
    "        self.V = np.zeros(self.env.observation_space.n)\n",
    "        \n",
    "    def reconocimiento(self, contador): # Este método se encarga de reconocer el entorno y de actualizar las estimaciones de recompensa y transiciones\n",
    "        for n in range(contador):\n",
    "            accion = self.env.action_space.sample()\n",
    "            nuevo_estado, recompensa, is_done, truncado, info = self.env.step(accion) # Se realiza una acción aleatoria\n",
    "            self.recompensas[(self.state, accion, nuevo_estado)] = recompensa\n",
    "            self.transiciones[(self.state, accion)][nuevo_estado] += 1\n",
    "            if is_done:\n",
    "                self.state = self.env.reset()[0]\n",
    "            else:\n",
    "                self.state = nuevo_estado\n",
    "                \n",
    "    def calcular_Q(self, estado, accion):\n",
    "        contador_transiciones = self.transiciones[(estado, accion)]\n",
    "        total = sum(contador_transiciones.values()) # Suma de todas las transiciones desde el estado y acción actuales\n",
    "        valor_accion = 0.0\n",
    "        \n",
    "        for siguiente_estado, contador in contador_transiciones.items():\n",
    "            r = self.recompensas[(estado, accion, siguiente_estado)] # Se obtiene la recompensa estimada\n",
    "            prob = (contador / total)                               # Se obtiene la probabilidad estimada\n",
    "            valor_accion += prob * (r + self.GAMMA * self.V[siguiente_estado]) # Calcular Q\n",
    "        \n",
    "        return valor_accion\n",
    "    \n",
    "    def seleccionar_accion(self, s):\n",
    "        mejor_accion = mejor_valor = False\n",
    "        \n",
    "        for accion in range(self.env.action_space.n): # Para cada acción (son 4)\n",
    "            valor_accion = self.calcular_Q(s, accion)\n",
    "            if mejor_valor is False or mejor_valor < valor_accion: # Si no hay mejor valor o el valor de la acción es mayor que el mejor valor\n",
    "                mejor_valor = valor_accion\n",
    "                mejor_accion = accion\n",
    "\n",
    "        return mejor_accion\n",
    "    \n",
    "    def Value_Iteration(self, contador=100):\n",
    "        self.reconocimiento(contador)\n",
    "        for estado in range(self.env.observation_space.n):\n",
    "            valores_estado = [self.calcular_Q(estado, accion) for accion in range(self.env.action_space.n)]\n",
    "            self.V[estado] = max(valores_estado)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor recompensa actualizada 0.25 en la iteración 11\n",
      "Mejor recompensa actualizada 0.40 en la iteración 21\n",
      "Mejor recompensa actualizada 0.50 en la iteración 23\n",
      "Mejor recompensa actualizada 0.75 en la iteración 25\n",
      "Mejor recompensa actualizada 0.80 en la iteración 27\n",
      "Mejor recompensa actualizada 0.90 en la iteración 40\n"
     ]
    }
   ],
   "source": [
    "UMBRAL_RECOMPENSA = 0.9\n",
    "t = 0\n",
    "mejor_recompensa = 0\n",
    "agenteEsti = AgenteEstimaciones(GAMMA=0.95)\n",
    "\n",
    "# BUCLE DE ENTRENAMIENTO\n",
    "while mejor_recompensa < UMBRAL_RECOMPENSA:\n",
    "    agenteEsti.Value_Iteration()\n",
    "    t += 1\n",
    "    recompensa_total_episodios = revisar_mejoras(agenteEsti)\n",
    "    \n",
    "    if recompensa_total_episodios > mejor_recompensa:\n",
    "        print(f\"Mejor recompensa actualizada {recompensa_total_episodios:.2f} en la iteración {t}\")\n",
    "        mejor_recompensa = recompensa_total_episodios"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar algunos episodios del agente ya entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revisar_mejoras_render(agenteEsti, no_episodios=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar las estimaciones de recompensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {(0, 1, 1): 0.0,\n",
       "             (1, 1, 5): 0.0,\n",
       "             (0, 0, 4): 0.0,\n",
       "             (4, 3, 5): 0.0,\n",
       "             (0, 0, 0): 0.0,\n",
       "             (1, 2, 1): 0.0,\n",
       "             (1, 0, 0): 0.0,\n",
       "             (0, 3, 0): 0.0,\n",
       "             (1, 2, 2): 0.0,\n",
       "             (2, 0, 2): 0.0,\n",
       "             (2, 2, 6): 0.0,\n",
       "             (6, 2, 7): 0.0,\n",
       "             (0, 1, 0): 0.0,\n",
       "             (0, 2, 0): 0.0,\n",
       "             (0, 3, 1): 0.0,\n",
       "             (1, 0, 5): 0.0,\n",
       "             (0, 1, 4): 0.0,\n",
       "             (4, 0, 4): 0.0,\n",
       "             (4, 1, 4): 0.0,\n",
       "             (4, 1, 5): 0.0,\n",
       "             (0, 2, 1): 0.0,\n",
       "             (1, 1, 2): 0.0,\n",
       "             (2, 1, 1): 0.0,\n",
       "             (1, 3, 1): 0.0,\n",
       "             (1, 3, 0): 0.0,\n",
       "             (1, 2, 5): 0.0,\n",
       "             (2, 1, 6): 0.0,\n",
       "             (0, 2, 4): 0.0,\n",
       "             (4, 2, 0): 0.0,\n",
       "             (1, 0, 1): 0.0,\n",
       "             (4, 3, 0): 0.0,\n",
       "             (4, 2, 5): 0.0,\n",
       "             (6, 1, 10): 0.0,\n",
       "             (10, 3, 11): 0.0,\n",
       "             (4, 2, 8): 0.0,\n",
       "             (8, 1, 12): 0.0,\n",
       "             (6, 3, 7): 0.0,\n",
       "             (4, 0, 0): 0.0,\n",
       "             (4, 3, 4): 0.0,\n",
       "             (2, 2, 2): 0.0,\n",
       "             (2, 1, 3): 0.0,\n",
       "             (3, 3, 3): 0.0,\n",
       "             (3, 1, 2): 0.0,\n",
       "             (2, 2, 3): 0.0,\n",
       "             (3, 0, 3): 0.0,\n",
       "             (3, 2, 7): 0.0,\n",
       "             (4, 1, 8): 0.0,\n",
       "             (8, 3, 9): 0.0,\n",
       "             (9, 1, 13): 0.0,\n",
       "             (13, 2, 9): 0.0,\n",
       "             (9, 2, 5): 0.0,\n",
       "             (4, 0, 8): 0.0,\n",
       "             (8, 1, 8): 0.0,\n",
       "             (8, 0, 8): 0.0,\n",
       "             (8, 3, 4): 0.0,\n",
       "             (1, 1, 0): 0.0,\n",
       "             (2, 3, 3): 0.0,\n",
       "             (3, 2, 3): 0.0,\n",
       "             (8, 2, 12): 0.0,\n",
       "             (8, 2, 4): 0.0,\n",
       "             (1, 3, 2): 0.0,\n",
       "             (8, 0, 4): 0.0,\n",
       "             (8, 2, 9): 0.0,\n",
       "             (9, 3, 5): 0.0,\n",
       "             (8, 3, 8): 0.0,\n",
       "             (2, 0, 6): 0.0,\n",
       "             (6, 1, 7): 0.0,\n",
       "             (9, 0, 8): 0.0,\n",
       "             (8, 0, 12): 0.0,\n",
       "             (6, 0, 10): 0.0,\n",
       "             (10, 1, 14): 0.0,\n",
       "             (14, 1, 14): 0.0,\n",
       "             (14, 3, 10): 0.0,\n",
       "             (10, 0, 6): 0.0,\n",
       "             (6, 0, 2): 0.0,\n",
       "             (9, 1, 10): 0.0,\n",
       "             (10, 2, 11): 0.0,\n",
       "             (2, 3, 1): 0.0,\n",
       "             (10, 1, 9): 0.0,\n",
       "             (13, 0, 9): 0.0,\n",
       "             (9, 1, 8): 0.0,\n",
       "             (8, 1, 9): 0.0,\n",
       "             (6, 3, 2): 0.0,\n",
       "             (3, 0, 2): 0.0,\n",
       "             (2, 0, 1): 0.0,\n",
       "             (2, 3, 2): 0.0,\n",
       "             (3, 1, 3): 0.0,\n",
       "             (9, 3, 10): 0.0,\n",
       "             (10, 3, 6): 0.0,\n",
       "             (3, 3, 2): 0.0,\n",
       "             (3, 0, 7): 0.0,\n",
       "             (6, 2, 10): 0.0,\n",
       "             (3, 1, 7): 0.0,\n",
       "             (6, 0, 5): 0.0,\n",
       "             (10, 1, 11): 0.0,\n",
       "             (9, 2, 13): 0.0,\n",
       "             (13, 1, 13): 0.0,\n",
       "             (13, 0, 13): 0.0,\n",
       "             (13, 1, 12): 0.0,\n",
       "             (10, 0, 14): 0.0,\n",
       "             (14, 0, 14): 0.0,\n",
       "             (14, 0, 13): 0.0,\n",
       "             (13, 2, 14): 0.0,\n",
       "             (14, 0, 10): 0.0,\n",
       "             (10, 2, 14): 0.0,\n",
       "             (13, 2, 13): 0.0,\n",
       "             (9, 2, 10): 0.0,\n",
       "             (14, 1, 15): 1.0,\n",
       "             (6, 2, 2): 0.0,\n",
       "             (6, 1, 5): 0.0,\n",
       "             (10, 3, 9): 0.0,\n",
       "             (6, 3, 5): 0.0,\n",
       "             (9, 0, 5): 0.0,\n",
       "             (14, 2, 14): 0.0,\n",
       "             (14, 2, 10): 0.0,\n",
       "             (14, 1, 13): 0.0,\n",
       "             (13, 0, 12): 0.0,\n",
       "             (14, 2, 15): 1.0,\n",
       "             (13, 3, 14): 0.0,\n",
       "             (9, 3, 8): 0.0,\n",
       "             (9, 0, 13): 0.0,\n",
       "             (13, 3, 9): 0.0,\n",
       "             (13, 3, 12): 0.0,\n",
       "             (13, 1, 14): 0.0,\n",
       "             (10, 2, 6): 0.0,\n",
       "             (10, 0, 9): 0.0,\n",
       "             (14, 3, 15): 1.0})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agenteEsti.recompensas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar las estimaciones de transición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {(0, 1): Counter({1: 126, 0: 132, 4: 135}),\n",
       "             (1, 1): Counter({5: 50, 2: 70, 0: 60}),\n",
       "             (0, 0): Counter({4: 113, 0: 264}),\n",
       "             (4, 3): Counter({5: 57, 0: 54, 4: 35}),\n",
       "             (1, 2): Counter({1: 60, 2: 50, 5: 66}),\n",
       "             (1, 0): Counter({0: 48, 5: 44, 1: 41}),\n",
       "             (0, 3): Counter({0: 267, 1: 140}),\n",
       "             (2, 0): Counter({2: 30, 6: 28, 1: 30}),\n",
       "             (2, 2): Counter({6: 21, 2: 33, 3: 27}),\n",
       "             (6, 2): Counter({7: 8, 10: 4, 2: 11}),\n",
       "             (0, 2): Counter({0: 151, 1: 142, 4: 162}),\n",
       "             (4, 0): Counter({4: 43, 0: 54, 8: 51}),\n",
       "             (4, 1): Counter({4: 53, 5: 45, 8: 59}),\n",
       "             (2, 1): Counter({1: 31, 6: 21, 3: 31}),\n",
       "             (1, 3): Counter({1: 65, 0: 51, 2: 50}),\n",
       "             (4, 2): Counter({0: 48, 5: 45, 8: 53}),\n",
       "             (2, 3): Counter({3: 30, 1: 20, 2: 25}),\n",
       "             (3, 0): Counter({3: 18, 2: 18, 7: 11}),\n",
       "             (3, 1): Counter({2: 17, 3: 15, 7: 9}),\n",
       "             (3, 2): Counter({7: 19, 3: 45}),\n",
       "             (3, 3): Counter({3: 30, 2: 14}),\n",
       "             (5, 0): Counter(),\n",
       "             (5, 1): Counter(),\n",
       "             (5, 2): Counter(),\n",
       "             (5, 3): Counter(),\n",
       "             (6, 0): Counter({10: 10, 2: 6, 5: 5}),\n",
       "             (6, 1): Counter({10: 16, 7: 6, 5: 6}),\n",
       "             (6, 3): Counter({7: 6, 2: 3, 5: 4}),\n",
       "             (7, 0): Counter(),\n",
       "             (7, 1): Counter(),\n",
       "             (7, 2): Counter(),\n",
       "             (7, 3): Counter(),\n",
       "             (8, 0): Counter({8: 29, 4: 17, 12: 19}),\n",
       "             (8, 1): Counter({12: 25, 8: 30, 9: 22}),\n",
       "             (8, 2): Counter({12: 20, 4: 16, 9: 23}),\n",
       "             (8, 3): Counter({9: 22, 4: 24, 8: 21}),\n",
       "             (9, 0): Counter({8: 11, 5: 6, 13: 4}),\n",
       "             (9, 1): Counter({13: 12, 10: 11, 8: 7}),\n",
       "             (9, 2): Counter({5: 6, 13: 9, 10: 8}),\n",
       "             (9, 3): Counter({5: 7, 10: 5, 8: 7}),\n",
       "             (10, 0): Counter({6: 5, 14: 4, 9: 3}),\n",
       "             (10, 1): Counter({14: 9, 9: 3, 11: 8}),\n",
       "             (10, 2): Counter({11: 3, 14: 4, 6: 3}),\n",
       "             (10, 3): Counter({11: 10, 6: 7, 9: 9}),\n",
       "             (11, 0): Counter(),\n",
       "             (11, 1): Counter(),\n",
       "             (11, 2): Counter(),\n",
       "             (11, 3): Counter(),\n",
       "             (12, 0): Counter(),\n",
       "             (12, 1): Counter(),\n",
       "             (12, 2): Counter(),\n",
       "             (12, 3): Counter(),\n",
       "             (13, 0): Counter({9: 6, 13: 5, 12: 1}),\n",
       "             (13, 1): Counter({13: 1, 12: 6, 14: 3}),\n",
       "             (13, 2): Counter({9: 4, 14: 3, 13: 2}),\n",
       "             (13, 3): Counter({14: 4, 9: 1, 12: 3}),\n",
       "             (14, 0): Counter({14: 5, 13: 3, 10: 6}),\n",
       "             (14, 1): Counter({14: 3, 15: 2, 13: 3}),\n",
       "             (14, 2): Counter({14: 5, 10: 4, 15: 4}),\n",
       "             (14, 3): Counter({10: 4, 15: 1}),\n",
       "             (15, 0): Counter(),\n",
       "             (15, 1): Counter(),\n",
       "             (15, 2): Counter(),\n",
       "             (15, 3): Counter()})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agenteEsti.transiciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

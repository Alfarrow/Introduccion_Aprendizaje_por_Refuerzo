{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método REINFORCE\n",
    "El método **REINFORCE**, es una técnica de aprendizaje por refuerzo *policy-based*, esto significa que se entrena una red neuronal para que nos indique cuál puede ser la siguiente acción a realizar, a diferencia de DQN (de la práctica anterior), donde se busca generar los valores de las acciones. En resumen, este método recibe el estado del entorno y retorna directamente la probabilidad de cada acción, por lo que, lo que se busca en el entrenamiento, es hacer que se más probable seleccionar la acción óptima en cada estado, o sea la política óptima.\n",
    "\n",
    "Características:\n",
    "- No se usan valores de acción o estado.\n",
    "- Es adecuado para espacios de acción continua (en general todos los policy-based).\n",
    "- Es un método *on-policy*, pues los datos del entorno se obtienen siguiendo la misma política que se intenta optimizar.\n",
    "- Se basa en trayectorias ($\\tau$) en lugar de episodios.\n",
    "\n",
    "Ventajas:\n",
    "- No es necesario preocuparse por idear una estrategia de exploración del entorno como *epsilon-greedy*.\n",
    "- No es necesario usar 'trucos' tales como *experience replay* o *target network*.\n",
    "\n",
    "Desventajas:\n",
    "- Suele ser menos eficiente en cuanto a muestras y requiere una mayor interacción con el entorno, pues no se puede beneficiar de datos antiguos. (en general todos los policy based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_size = env.observation_space.shape[0]   # Entrada de la red\n",
    "n_actions = env.action_space.n              # Salida de la red\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(obs_size, HIDDEN_SIZE),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(HIDDEN_SIZE, n_actions),\n",
    "    torch.nn.Softmax(dim=0) \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiperparámetros de la red y del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros de la red\n",
    "LEARNING_RATE = 0.003\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Hiperparámetros del algoritmo\n",
    "HORIZON = 500\n",
    "MAX_TRAJECTORIES = 500\n",
    "GAMMA = 0.99\n",
    "\n",
    "score = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "#### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generar una trayectoria\n",
    "def GenerarTrayectoria(model, estado_actual, horizonte):\n",
    "    done = False\n",
    "    transiciones = []\n",
    "    \n",
    "    for t in range(horizonte):\n",
    "        prob_acciones = model(torch.from_numpy(estado_actual).float())         # Se obtienen las probabilidades de las acciones\n",
    "        accion = np.random.choice(np.array([0,1]), p=prob_acciones.data.numpy()) # Se escoge una acción pero con las probabilidades de la red\n",
    "        estado_anterior = estado_actual\n",
    "        estado_actual, recompensa, done, trunc, info = env.step(accion)        # Se ejecuta la acción\n",
    "        transiciones.append((estado_anterior, accion, t+1))\n",
    "        if done is True:\n",
    "            break\n",
    "    \n",
    "    return transiciones\n",
    "\n",
    "# Función para calcular el retorno de una trayectoria\n",
    "def Rt(batch_recompensas, transiciones, GAMMA):\n",
    "    batch_Gvals = []\n",
    "    \n",
    "    for n in range(len(transiciones)):\n",
    "        new_Gval = 0\n",
    "        power = 0\n",
    "        for m in range(n, len(transiciones)):\n",
    "            new_Gval = new_Gval + ((GAMMA**power)*batch_recompensas[m]).numpy()\n",
    "            power += 1\n",
    "        batch_Gvals.append(new_Gval)\n",
    "    batch_retornos_estimados = torch.tensor(batch_Gvals)\n",
    "    batch_retornos_estimados /= batch_retornos_estimados.max()     # Se normalizan los retornos para que no haya problemas de estabilidad numérica en la red\n",
    "    \n",
    "    return batch_retornos_estimados   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método REINFORCE es un algoritmo *policy gradient*, esto significa que actualiza una red neuronal usando el **ascenso de gradiente**. Los pasos a seguir junto con sus ecuaciones se muestran a continuación:\n",
    "1. Se usa la política $\\pi_{\\theta}$ para generar una trayectoria (recordar que la política viene dada por la red neuronal $\\theta$, por eso se escribe $\\pi_{\\theta}$).\n",
    "2. Se estiman las recompensas en la trayectoria $R(\\tau) = (G_{0}, G_{1},..., G_{H})$. Cada $G_{k}$ se calcula con la fórmula: $G_{k}=\\sum_{i=k+1}^{H+1} \\gamma^{i-k-1}r_{i}$\n",
    "3. Se estima el gradiente usando $\\nabla_{\\theta} = \\sum_{t=0}^{H} \\nabla_{\\theta}log\\pi_{\\theta}(a_{t}|s_{t})G_{t}$. Se observa que se multiplican las probabilidades de cada acción en un estado $\\pi_{\\theta}(a_{t}|s_{t})$ con su retorno esperado $G_{t}$\n",
    "4. Se actualizan los parámetros de la red neuronal usando: $\\theta = \\theta + \\alpha \\nabla_{\\theta}U(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_12664\\1884484331.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  batch_estados = torch.Tensor([s for (s,a,r) in transiciones])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory 50\tAverage Score: 47.92\n",
      "Trajectory 100\tAverage Score: 67.35\n",
      "Trajectory 150\tAverage Score: 97.61\n",
      "Trajectory 200\tAverage Score: 151.86\n",
      "Trajectory 250\tAverage Score: 124.47\n",
      "Trajectory 300\tAverage Score: 224.24\n",
      "Trajectory 350\tAverage Score: 352.14\n",
      "Trajectory 400\tAverage Score: 434.43\n",
      "Trajectory 450\tAverage Score: 383.27\n"
     ]
    }
   ],
   "source": [
    "for trajectory in range(MAX_TRAJECTORIES):\n",
    "    estado_actual = env.reset()[0]\n",
    "    done = False\n",
    "    \n",
    "    # Generar una trayectoria\n",
    "    transiciones = GenerarTrayectoria(model, estado_actual, HORIZON)\n",
    "    score.append(len(transiciones))                                                   # Se guarda el score de la trayectoria\n",
    "    batch_recompensas = torch.Tensor([r for (s,a,r) in transiciones]).flip(dims=(0,)) # Se crea un tensor con las recompensas de la trayectoria\n",
    "    \n",
    "    # Se calculan el retorno de la trayectoria R(t) = (G0,G1,...,GH)\n",
    "    batch_retornos_estimados = Rt(batch_recompensas, transiciones, GAMMA)\n",
    "    \n",
    "    # Batch de estados y acciones\n",
    "    batch_estados = torch.Tensor([s for (s,a,r) in transiciones])\n",
    "    batch_acciones = torch.Tensor([a for (s,a,r) in transiciones])\n",
    "    \n",
    "    # Se predice la probabilidad de las acciones con la red neuronal\n",
    "    predicted_batch = model(batch_estados)\n",
    "    batch_prob = predicted_batch.gather(dim=1, index=batch_acciones.long().view(-1,1)).squeeze() # Se seleccionan las probabilidades de las acciones que se tomaron antes\n",
    "    \n",
    "    # Se calcula la pérdida y se actualizan los pesos\n",
    "    loss = -torch.sum(torch.log(batch_prob)*batch_retornos_estimados)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if trajectory % 50 == 0 and trajectory>0:\n",
    "        print('Trajectory {}\\tAverage Score: {:.2f}'.format(trajectory, np.mean(score[-50:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo\n",
    "torch.save(model.state_dict(), \"Practica8_REINFORCE_Net.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
